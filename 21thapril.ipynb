{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac24461-9502-4850-90ce-941502e70b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "  The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is the way distance is calculated between two points in a multi-dimensional space.\n",
    "Euclidean distance is the straight-line distance between two points and is calculated as the square root of the sum of the squared differences between each coordinate axis.\n",
    "Manhattan distance, on the other hand, is the distance between two points measured along the axes at right angles. It is calculated as the sum of the absolute differences between each coordinate axis.\n",
    "The choice of distance metric can affect the performance of a KNN classifier or regressor. Euclidean distance may work better when the data is well-spread out, while Manhattan distance may work better\n",
    "when the data is clustered. Additionally, the choice of distance metric may depend on the nature of the problem being solved.\n",
    "In summary, the choice of distance metric can impact the accuracy of the KNN classifier or regressor, and it is important to experiment with different distance metrics to determine which one works best for a given problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45169214-7fc3-44c7-8eea-5fca1cb5114a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4f1e4-20c2-48a3-9d8f-793abad52538",
   "metadata": {},
   "outputs": [],
   "source": [
    "2:\n",
    "   Choosing the optimal value of k for a KNN classifier or regressor is a critical step in building an accurate\n",
    "model. One common approach to determine the optimal k value is to use cross-validation. In this approach, the data\n",
    "is split into k-folds and the model is trained on k-1 folds and tested on the remaining fold. This process is\n",
    "repeated for all possible combinations of folds, and the average accuracy or error rate is computed for each \n",
    "value of k. The k value that yields the highest accuracy or lowest error rate is then selected as the optimal value of k.\n",
    "   Another approach to determine the optimal k value is to use a grid search, where a range of k values are tested, \n",
    "and the optimal value is determined based on the performance metrics such as accuracy or error rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b30beb-74ae-4b7e-875d-f78c103922f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99eba4-5a2b-47d5-b74e-189d82c7f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "3:\n",
    "  The choice of distance metric can significantly affect the performance of a KNN classifier or regressor. \n",
    "Euclidean distance is commonly used when the data is well-spread out and continuous, while Manhattan distance \n",
    "is used when the data is clustered or discrete. Additionally, other distance metrics such as Minkowski distance\n",
    "or Cosine similarity can be used in specific situations.\n",
    "For example, if the data contains both continuous and categorical features, the Gower distance metric can be used,\n",
    "which is a mixture of Euclidean and Manhattan distance. In text classification, Cosine similarity is commonly used\n",
    "as it takes into account the angle between two vectors, which can be more meaningful than the distance between them.\n",
    "In general, the choice of distance metric depends on the nature of the problem being solved and the characteristics\n",
    "of the data. It is important to experiment with different distance metrics to determine which one works best for a given problem.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff39e-c820-4e1f-826f-3752bd0f027d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c12459-903b-4049-86a9-e10c21d50ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "4:\n",
    " Some common hyperparameters in KNN classifiers and regressors include:\n",
    "1.n_neighbors: the number of neighbors to consider when making a prediction.\n",
    "\n",
    "2.weights: the weight function used in prediction. Options include uniform (all neighbors are weighted equally) \n",
    "and distance (neighbors are weighted by their distance to the query point).\n",
    "\n",
    "3.p: the power parameter used in the distance metric. For example,\n",
    "  p=1\n",
    "corresponds to using the Manhattan distance, while\n",
    "  p=2\n",
    "corresponds to using the Euclidean distance.\n",
    "\n",
    "The choice of hyperparameters can have a significant impact on the performance of the model. \n",
    "For example, increasing n_neighbors can lead to a smoother decision boundary, while decreasing\n",
    "n_neighbors can lead to a more complex decision boundary. Similarly, using the distance weight \n",
    "function can give more weight to closer neighbors, while using the uniform weight function gives\n",
    "equal weight to all neighbors.\n",
    "\n",
    "To tune these hyperparameters, one approach is to use grid search or random search over a range\n",
    "of hyperparameter values to find the combination that yields the best performance on a validation set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bc67b-4aaf-4fbf-96b8-e6f9ead35c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd103ea7-7e6c-4fe9-98cb-3ce36679daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    "  The size of the training set can affect the performance of a KNN classifier or regressor. A larger training\n",
    "set can lead to better performance, as the model has more data to learn from and is less likely to overfit. \n",
    "However, a larger training set also means that the model may be slower to train and may require more memory \n",
    "to store.\n",
    "To optimize the size of the training set, one approach is to use cross-validation to estimate the performance \n",
    "of the model on different sizes of training sets. This can help identify the smallest training set that still\n",
    "yields good performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb34c86-b447-443e-a1ba-d1376dfcc6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52953e-77f4-4306-b4e4-c4dc29e6a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "6:\n",
    "  'Some potential drawbacks of using KNN as a classifier or regressor include:\n",
    "1.High memory usage: KNN requires storing the entire training set, which can be memory-intensive for large datasets.\n",
    "2.Slow prediction: KNN requires computing the distance between the query point and all training points, which can be slow for large datasets.\n",
    "3.Sensitive to irrelevant features: KNN considers all features equally when computing distances, which can lead to poor performance if some features are irrelevant.\n",
    "\n",
    "To overcome these drawbacks, one approach is to use dimensionality reduction techniques to reduce the number of features in the dataset.\n",
    "Additionally, using distance metrics that are more robust to irrelevant features, such as cosine distance, can improve performance. \n",
    "Finally, using approximate nearest neighbor algorithms, such as locality-sensitive hashing, can speed up the prediction phase of KNN.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27153937-ff7d-44f2-aeaa-5d389933db82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcbe7b-cd09-4e73-844f-f0c7ef275bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3348eb1-bafe-4509-847e-b81cd9cfc831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969155f-e30b-41e5-9035-f2f894abf9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268bf27-8bec-42db-9980-311d66e3017f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
